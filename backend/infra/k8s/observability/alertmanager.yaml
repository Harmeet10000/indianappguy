apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: observability
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'alerts@platform.com'
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

    route:
      group_by: ['alertname']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
      routes:
      - match:
          severity: critical
        receiver: pagerduty-critical
      - match:
          severity: warning
        receiver: pagerduty-warning

    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://127.0.0.1:5001/'

    - name: 'pagerduty-critical'
      pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY'
        description: 'Critical Alert: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        severity: 'critical'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          cluster: 'platform-production'

    - name: 'pagerduty-warning'
      pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY'
        description: 'Warning Alert: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        severity: 'warning'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          cluster: 'platform-production'

    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'dev', 'instance']
---
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: alertmanager-main
  namespace: observability
  labels:
    app: alertmanager
spec:
  replicas: 3
  resources:
    requests:
      memory: 100Mi
      cpu: 100m
    limits:
      memory: 200Mi
      cpu: 200m
  storage:
    volumeClaimTemplate:
      spec:
        storageClassName: gp3
        accessModes: ['ReadWriteOnce']
        resources:
          requests:
            storage: 5Gi
  securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  configSecret: alertmanager-config-secret
---
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-config-secret
  namespace: observability
type: Opaque
stringData:
  alertmanager.yaml: |
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'alerts@platform.com'
      pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

    route:
      group_by: ['alertname']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
      routes:
      - match:
          severity: critical
        receiver: pagerduty-critical
      - match:
          severity: warning
        receiver: pagerduty-warning

    receivers:
    - name: 'web.hook'
      webhook_configs:
      - url: 'http://127.0.0.1:5001/'

    - name: 'pagerduty-critical'
      pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY'
        description: 'Critical Alert: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        severity: 'critical'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          cluster: 'platform-production'

    - name: 'pagerduty-warning'
      pagerduty_configs:
      - routing_key: 'YOUR_PAGERDUTY_INTEGRATION_KEY'
        description: 'Warning Alert: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
        severity: 'warning'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          cluster: 'platform-production'

    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'dev', 'instance']
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager-main
  namespace: observability
  labels:
    app: alertmanager
spec:
  ports:
    - name: web
      port: 9093
      targetPort: web
  selector:
    app.kubernetes.io/name: alertmanager
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: platform-alerts
  namespace: observability
  labels:
    team: platform
spec:
  groups:
    - name: kubernetes-apps
      rules:
        - alert: KubePodCrashLooping
          expr: max_over_time(increase(kube_pod_container_status_restarts_total[1h])[5m:1m]) > 0
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: Pod is crash looping
            description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.'

        - alert: KubePodNotReady
          expr: sum by (namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown"}) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"}))) > 0
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: Pod has been in a non-ready state for more than 15 minutes
            description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.'

    - name: kubernetes-resources
      rules:
        - alert: KubeNodeNotReady
          expr: kube_node_status_condition{condition="Ready",status="true"} == 0
          for: 15m
          labels:
            severity: critical
          annotations:
            summary: Node is not ready
            description: 'Node {{ $labels.node }} has been unready for more than 15 minutes.'

        - alert: KubeMemoryOvercommit
          expr: (sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum{}) - sum(kube_node_status_allocatable_memory_bytes) + sum(kube_node_status_allocatable_memory_bytes)) / sum(kube_node_status_allocatable_memory_bytes) > 1.5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: Cluster has overcommitted memory resource requests
            description: 'Cluster has overcommitted memory resource requests for Pods by {{ $value }} and cannot tolerate node failure.'
